---
sidebar_position: 1
sidebar_label: 'Service overview'
title: 'Service overview'
slug: '/user-guides/twcc/ccs-interactive-container/overview'
sync_original_production: 'https://man.twcc.ai/@twccdocs/ccs-overview-zh' 
sync_original_preview: 'https://man.twcc.ai/@preview-twccdocs/ccs-overview-zh' 
---


# <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_0b81080da8a39866cd1e0aa0471e9552.png" width="25" height="25"/> Service overview

<br/>

## Container Compute Service provides rapid deployment of GPU working environment and improves work efficiency!

Interactive Container enables you to deploy development environment quickly, choose different AI frameworks on demand, and edit scripts directly through the built-in Jupyter Notebook. Container service is mounted with Hyper File System (HFS) automatically. Also, users can use Cloud Object Storage(COS) in containers by themselves to store training data and models.

<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_9031b03afa1291f2d95f9dbc60cf2948.png" width="35" height="25"/> Use the service with ease

Container service may be established through TWCC portal, API, or CLI (Command Line Interface). Unlike traditional supercomputer resources that can only be operated via CLI, TWCC gives you the freedom to choose an interface that you are familiar with and to stack applications easily.


<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_afd344f9a1b3d0567f83a250da8b8d26.png" width="35" height="25"/> Rapidly deploy a working environment

Using Kubernetes open-source framework, introducing optimized Nvidia AI software stack and utilizing world-connecting virtual technologies, a work environment can be deployed rapidly in a short period of time, saving up to 3 times the time compared to traditional approaches and allowing users the flexibility to switch platforms.


<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_d404fdf4e28033ae3c6185c87888ab51.png" width="35" height="25"/> Diverse AI frameworks

TWCC provides users with Nvidia-optimized AI frameworks such as TensorFlow, PyTorch, CUDA, Matlab (BYOL), Caffe, CNTK, MXNet, Caffe2, TensorRT, Triton Inference Server, Theano, Torch, DIGITS, NeMo, Clara Train SDK, RAPIDS, Merlin Training and Merlin Inference, which can be installed effortlessly to meet the needs of different model training and inference.


<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_cb712cc256270388197b36fdb9757d68.png" width="35" height="25"/> Different container options

- Interactive containers are coupled with Jupyter Notebook editor for programs enable online debugging and computation. Work can be completed even with mobile devices so that you always have a firm grasp of the progress of your project.
- Scheduled containers allow the designation of job starting time, so users can appropriately schedule the amount of resources by batches and improve work efficiency. Once the computations are completed, computing resources are taken back, effectively saving the costs of development.


<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_22d79d4fc2df0425c3f9c9e1e0591396.png" width="35" height="25"/> Safe and fast storage system

TWCC provides Hyper File System with high throughput to accelerate computations. You may also use Cloud Object Storage. Your data can be stored safely so you donâ€™t need to worry about losing them.


<br/>

### <img src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_b5b69c51d21e91a714e4152465fed59e.png" width="35" height="25"/> Multiple bottom-layer benefits

- Network structure for tenant isolation makes it safe to use.
- Functions are constantly optimized with Kubernetes rolling updates.
- With Kubernetes and HPC hyper-converged infrastructure (HCI) , GPU containers for application development have the capability to allocate resources in HPC for parallel computing.
