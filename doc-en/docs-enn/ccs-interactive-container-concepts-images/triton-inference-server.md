---
sidebar_label: 'Triton Inference Server'
sidebar_position: 11
title: 'Triton Inference Server'
sync_original_production: 'https://man.twcc.ai/@twccdocs/ccs-concept-image-tensorrtserver-en'
sync_original_preview: 'https://man.twcc.ai/@preview-twccdocs/ccs-concept-image-tensorrtserver-en'
---

#  <img style={{width:55+'px'}} src='https://cos.twcc.ai/SYS-MANUAL/uploads/upload_f55059e9d0a6ac45c44bcc0ec1bebff5.png' /> Triton Inference Server

TWCC provides pay-as-you-go working environment of NGCâ€™s TensorRT Inference Server. The TensorRT inference server provides an inference service via an HTTP endpoint, allowing remote clients to request inferencing for any model that is being managed by the server. The TensorRT inference server itself is included in the TensorRT inference server container. External to the container, there are additional C++ and Python client libraries, and additional documentation at GitHub: Inference Server.

## <i class="fa fa-sticky-note" aria-hidden="true"></i> <span class="ccsimglist">Information of Image file version</span> 

![](https://cos.twcc.ai/SYS-MANUAL/uploads/upload_406447ea29f101fc48b37000ddd0fbe6.png)



:::info
`py3` and `py2` are Python version differences.
:::

<details class="docspoiler">

<summary><b>Detailed package version information</b></summary>

- [tritonserver-22.02-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_22-02.html#rel_22-02)
- [tritonserver-21.11-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-11.html#rel_21-11)
- [tritonserver-21.08-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-08.html#rel_21-08)
- [tritonserver-21.06-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-06.html#rel_21-06)
- [tritonserver-21.02-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-02.html#rel_21-02)
- [tensorrtserver-20.02-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_20-02.html#rel_20-02)
- [tensorrtserver-19.02-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_19-02.html#rel_19-02)
- [tensorrtserver-18.12-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.12.html#rel_18.12)
- [tensorrtserver-18.10-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.10.html#rel_18.10)
- [tensorrtserver-18.08.1-py2/py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.08.html#rel_18.08)

</details>


