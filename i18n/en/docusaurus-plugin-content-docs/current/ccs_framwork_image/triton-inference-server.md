---
sidebar_label: 'Triton Inference Server'
sidebar_position: 11
title: 'Triton '
sync_original_production: 'https://man.twcc.ai/@twccdocs/ccs-concept-image-tensorrtserver-en'
sync_original_preview: 'https://man.twcc.ai/@preview-twccdocs/ccs-concept-image-tensorrtserver-en'
---

#  <img style={{width:55+'px'}} src='https://cos.twcc.ai/SYS-MANUAL/uploads/upload_f55059e9d0a6ac45c44bcc0ec1bebff5.png' /> Triton Inference Server


TWCC provides ready-to-use working environment of NGC’s TensorRT Inference Server. The TensorRT inference server provides an inference service via an HTTP endpoint, allowing remote clients to request inferencing for any model that is being managed by the server. The TensorRT inference server itself is included in the TensorRT inference server container. External to the container, there are additional C++ and Python client libraries, and additional documentation at GitHub: Inference Server.

## <i class="fa fa-sticky-note" aria-hidden="true"></i> <span class="ccsimglist">Information of Image file version</span> 

![](https://cos.twcc.ai/SYS-MANUAL/uploads/upload_406447ea29f101fc48b37000ddd0fbe6.png)



:::info
`py3` 與 `py2` 為 Python 版本差異。
:::

<details class="docspoiler">

<summary><b>Detailed package version information</b></summary>

- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-02.html#rel_21-02">tritonserver-21.02-py3</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_20-02.html#rel_20-02">tensorrtserver-20.02-py3</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_19-02.html#rel_19-02">tensorrtserver-19.02-py3-v1</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.12.html#rel_18.12">tensorrtserver-18.12-py3-v1</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.10.html#rel_18.10">tensorrtserver-18.10-py3-v1</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.08.html#rel_18.08">tensorrtserver-18.08.1-py3-v1</a></ins>
- <ins><a href = "https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.08.html#rel_18.08">tensorrtserver-18.08.1-py2-v1</a></ins>

</details>


