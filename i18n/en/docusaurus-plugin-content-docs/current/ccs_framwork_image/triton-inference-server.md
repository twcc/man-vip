---
sidebar_label: 'Triton Inference Server'
sidebar_position: 11
---

# <img class="ccsimgicon" src="https://cos.twcc.ai/SYS-MANUAL/uploads/upload_f55059e9d0a6ac45c44bcc0ec1bebff5.png"> Triton Inference Server


TWCC provides ready-to-use working environment of NGCâ€™s TensorRT Inference Server. The TensorRT inference server provides an inference service via an HTTP endpoint, allowing remote clients to request inferencing for any model that is being managed by the server. The TensorRT inference server itself is included in the TensorRT inference server container. External to the container, there are additional C++ and Python client libraries, and additional documentation at GitHub: Inference Server.

## <i class="fa fa-sticky-note" aria-hidden="true"></i> <span class="ccsimglist">Information of Image file version
</span> 

![](https://cos.twcc.ai/SYS-MANUAL/uploads/upload_16ff54e48ef435764e0d3b0eb05e4b4e.png)



:::info
`py3` and `py2` are different Python versions.
:::

<details class="docspoiler">

<summary><b>Detailed package version information</b></summary>

- [tritonserver-21.02-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-02.html#rel_21-02)
- [tensorrtserver-20.02-py3](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_20-02.html#rel_20-02)
- [tensorrtserver-19.02-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_19-02.html#rel_19-02)
- [tensorrtserver-18.12-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.12.html#rel_18.12)
- [tensorrtserver-18.10-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.10.html#rel_18.10)
- [tensorrtserver-18.08.1-py3-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.08.html#rel_18.08)
- [tensorrtserver-18.08.1-py2-v1](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_18.08.html#rel_18.08)

</details>